{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict #defaultdict provides value of nonexist key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "REVIEW_DIR = \".\\data\\goodreads_reviews_comics_graphic.json\"\n",
    "BOOK_DIR = \".\\data\\goodreads_books_comics_graphic.json\"\n",
    "INTER_DIR = \".\\data\\goodreads_interactions_comics_graphic.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We need a user table which contains all the books he rated that has >=3\n",
    "# We need a list of all books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_review( record , table ):\n",
    "    \n",
    "    if record['rating']==0:\n",
    "        #do not perform any operation\n",
    "        return table\n",
    "    \n",
    "    \n",
    "    user_id = record['user_id']\n",
    "    book_id = record['book_id']\n",
    "    \n",
    "    \n",
    "    \n",
    "    if not book_id in table['books']:\n",
    "        table['books'].add(book_id)\n",
    "        #let table['books'] be a set\n",
    "    if not user_id in table.keys():  #check if this user has registered in our dataset\n",
    "        table[user_id] = set()\n",
    "\n",
    "    table[user_id].add(book_id) #register this book\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_records = 529532  #there are 529532 record in total\n",
    "index = 0 \n",
    "\n",
    "data = {'books':set()}\n",
    "\n",
    "#----------------------------\n",
    "#     run main\n",
    "#----------------------------\n",
    "\n",
    "with open(REVIEW_DIR) as fie:\n",
    "    for review in fie:\n",
    "        \n",
    "        if index > num_records:\n",
    "            fie.close()\n",
    "            break\n",
    "            \n",
    "        \n",
    "            \n",
    "        record = json.loads(review)  #load json as a dictionary\n",
    "        data = process_review(record, data)\n",
    "        \n",
    "        #print(i)\n",
    "        index+=1\n",
    "    \n",
    "    \n",
    "    \n",
    "    fie.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# we then need to build a look up dictionary for our books\n",
    "# we would first want to remove users with only one comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we then remove those user who only rates one book.\n",
    "data = [v for k,v in data.items() if len(v) > 1 ]\n",
    "data.pop(0) #also remove the first element, which is a set of all books\n",
    "# then compute how many books are in the dataset\n",
    "books = set()\n",
    "for v in data:\n",
    "    books = books.union(v)\n",
    "books = list(books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_up = {books[i]:i for i in range(len(books))  }  \n",
    "#this is a mapping dictionary that map book id to a unique id\n",
    "book_code = {i:books[i] for i in range(len(books))}\n",
    "#this is a book code dictionary that map id to book id back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we made up our training samples\n",
    "\n",
    "    Note here because the data is fairly large, we need a datagenerator to feed neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [list(i) for i in data] #change set to list\n",
    "data = [  [look_up[j] for j in i]  for i in data] #change all raw bookid to the id in look up table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [item for sublist in data for item in sublist] \n",
    "#flatten the data to a corpus, and we will use this corpus to do a counter job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2405, 3041, 71916, 43125, 25935]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookcounts = Counter(corpus)\n",
    "uniquebooks = np.unique(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negativeSampleTable(uniqueWords, wordcounts, exp_power=0.75): #exp_power is the default value\n",
    "    #global wordcounts\n",
    "    #... stores the normalizing denominator (count of all tokens, each count raised to exp_power)\n",
    "    max_exp_count = 0  #this is the sum of total weights\n",
    "    \n",
    "    print (\"Generating exponentiated count vectors\")\n",
    "    #... (TASK) for each uniqueWord, compute the frequency of that word to the power of exp_power\n",
    "    #... store results in exp_count_array.\n",
    "    exp_count_array = [wordcounts[i]**exp_power for i in wordcounts]\n",
    "    max_exp_count = sum(exp_count_array)\n",
    "\n",
    "    print (\"Generating distribution\")\n",
    "\n",
    "    #... (TASK) compute the normalized probabilities of each term.\n",
    "    #... using exp_count_array, normalize each value by the total value max_exp_count so that\n",
    "    #... they all add up to 1. Store this corresponding array in prob_dist\n",
    "    prob_dist = [np.float(i/max_exp_count) for i in exp_count_array]\n",
    "    #print(sum(prob_dist))\n",
    "\n",
    "\n",
    "    print (\"Filling up sampling table\")\n",
    "    #... (TASK) create a dict of size table_size where each key is a sequential number and its value is a one-hot index\n",
    "    #... the number of sequential keys containing the same one-hot index should be proportional to its prob_dist value\n",
    "    #... multiplied by table_size. This table should be stored in cumulative_dict.\n",
    "    #... we do this for much faster lookup later on when sampling from this table.\n",
    "\n",
    "    table_size = 1e7\n",
    "    counter=0 #this is to specify the index of array\n",
    "    #note if we do a for loop to print keys in a dict, it will preserve the same order for the same dict\n",
    "    #hence, we the order of above array prob_dist is the same as we print the wordcount dict\n",
    "    table_place = 0  #this is the sequential number we are current in among the dictionary cumulative_dict\n",
    "    cumulative_dict=dict()\n",
    "    \n",
    "    for key in wordcounts:\n",
    "        prob = prob_dist[counter]  #this will be the probability that it get sampled\n",
    "        \n",
    "        sub_table = round(prob*table_size) #round it to an int, which is how many keys in the table will have this index\n",
    "        \n",
    "        onehot_index = key\n",
    "        \n",
    "        for i in range(table_place,table_place+sub_table):  #we will make sub_table number of keys, they all have the same index.\n",
    "            cumulative_dict[i] = onehot_index\n",
    "        table_place = table_place+sub_table\n",
    "        counter+=1\n",
    "    \n",
    "\n",
    "    return cumulative_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating exponentiated count vectors\n",
      "Generating distribution\n",
      "Filling up sampling table\n"
     ]
    }
   ],
   "source": [
    "samplingTable = negativeSampleTable(uniquebooks, bookcounts)\n",
    "#samplingtable is a table that given a number between 0,10000 to will output the corresponding word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSamples(context_idx, num_samples):\n",
    "    #context_id is a list of ids which should not be negative sampled,\n",
    "    #num_samples is the total negative sample we want to do\n",
    "    global samplingTable\n",
    "    results = []\n",
    "\n",
    "    n=len(samplingTable)-1  #hence, we choose an int from 0 to n, which is the key of the sampling table\n",
    "    #... (TASK) randomly sample num_samples token indices from samplingTable.\n",
    "    #... don't allow the chosen token to be context_idx.\n",
    "    #... append the chosen indices to results\n",
    "    for i in range(num_samples):\n",
    "        index = context_idx #first make an index that will go the while loop       \n",
    "        while index in [context_idx]:  #this while loop will stop untile a sampled index is not in the context_idx\n",
    "            quantile = random.randint(0,n)\n",
    "            index = samplingTable[quantile]\n",
    "        #to get multiple negative samples, we will then add the selected samples into the context_idx\n",
    "        #this is like a sampling without replacement\n",
    "        results.append(index)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performDescent(num_samples, learning_rate, center_token, context_words,W1,W2,negative_indices):\n",
    "    # sequence chars was generated from the mapped sequence in the core code\n",
    "    nll_new = 0\n",
    "    #... (TASK) implement gradient descent. Find the current context token from context_words\n",
    "    #... and the associated negative samples from negative_indices. Run gradient descent on both\n",
    "    #... weight matrices W1 and W2.\n",
    "    #... compute the total negative log-likelihood and store this in nll_new.\n",
    "    #... You don't have to use all the input list above, feel free to change them\n",
    "    \n",
    "    voca = W1.shape[0]\n",
    "    hidden_size = W1.shape[1]\n",
    "    \n",
    "    #let's first train context_words\n",
    "\n",
    "    h = W1[center_token]\n",
    "    new_h = np.copy(h)\n",
    "    \n",
    "    #first let's solve for context_word\n",
    "    v_j = np.copy(W2[context_words])\n",
    "    sig = sigmoid( np.dot(v_j,np.transpose(h)) )\n",
    "    #minuse 1 because we are daling with context_words\n",
    "    #update log likelihood\n",
    "    nll_new-=np.log(sig)\n",
    "    #then update W1 \n",
    "    new_h -= learning_rate*(sig-1)*v_j\n",
    "    #then update the projection matrix\n",
    "    v_j = v_j - learning_rate*(sig-1)*h  \n",
    "    \n",
    "    W2[context_words]=v_j\n",
    "    \n",
    "    \n",
    "    #then we slove for negative words\n",
    "    for j in negative_indices:\n",
    "        v_j = W2[j]\n",
    "        sig = sigmoid( np.dot(v_j,np.transpose(h)) )\n",
    "        #update nll        \n",
    "        nsig = sigmoid(-np.dot(v_j,np.transpose(h)))\n",
    "        nll_new -= np.log(nsig)\n",
    "        #update W1\n",
    "        new_h -= learning_rate*sig*v_j\n",
    "        v_j = v_j - learning_rate*sig*h\n",
    "        W2[j] = v_j\n",
    "    #finally update h\n",
    "    W1[center_token] = new_h\n",
    "    \n",
    "\n",
    "    return nll_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(curW1 = None, curW2 = None, hidden_size=100):\n",
    "    global data #data is a list of list, in which each element is the id of that book.\n",
    "    \n",
    "    #... set the training parameters\n",
    "    epochs = 5\n",
    "    num_samples = 2\n",
    "    learning_rate = 0.05\n",
    "    nll = 0\n",
    "    iternum = 0\n",
    "    \n",
    "    book_size = len(look_up)\n",
    "    nll_results = []\n",
    "    \n",
    "    if curW1==None:\n",
    "        #np_randcounter += 1\n",
    "        W1 = np.random.uniform(-.5, .5, size=(book_size, hidden_size))\n",
    "        #print(W1)\n",
    "        W2 = np.random.uniform(-.5, .5, size=(book_size, hidden_size))\n",
    "        #print(W2)\n",
    "    else:\n",
    "        #... initialized from pre-loaded file\n",
    "        W1 = curW1\n",
    "        W2 = curW2  \n",
    "    \n",
    "    #now we start training:\n",
    "    for epc in range(epochs):\n",
    "        #how many epochs we need to run\n",
    "        print(\"epoch {epc}\".format(epc = str(epc)))\n",
    "        iternum=0\n",
    "        for user in data:\n",
    "            \n",
    "            if iternum%1000==0:\n",
    "                print (\"Negative likelihood: \", nll)\n",
    "                nll_results.append(nll)\n",
    "                nll = 0\n",
    "            \n",
    "            iternum += 1\n",
    "            #user is a list of all books this user rated\n",
    "            temp_books=user\n",
    "            #then we need to run through every book in this books set\n",
    "            for book_index in range(len(user)):\n",
    "                \n",
    "                center_book = temp_books[book_index]\n",
    "                context_books = temp_books[:book_index]+temp_books[(book_index+1):] \n",
    "                \n",
    "                #this is all contexts books, then we need to perform descent for each of this context books\n",
    "                for context_book in context_books:\n",
    "                    \n",
    "                    negative_indices = generateSamples(context_book, num_samples) #create negative samplings\n",
    "                    nll=performDescent(num_samples, learning_rate, center_book, context_book, W1, W2, negative_indices)\n",
    "                    #create negative log likelihood\n",
    "                    #nll_results.append(nll)\n",
    "                    \n",
    "                \n",
    "                    \n",
    "    return [W1,W2]\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(W1,W2):\n",
    "\thandle = open(\"saved_W1.data\",\"wb+\")\n",
    "\tnp.save(handle, W1, allow_pickle=False)\n",
    "\thandle.close()\n",
    "\n",
    "\thandle = open(\"saved_W2.data\",\"wb+\")\n",
    "\tnp.save(handle, W2, allow_pickle=False)\n",
    "\thandle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "\thandle = open(\"saved_W1.data\",\"rb\")\n",
    "\tW1 = np.load(handle)\n",
    "\thandle.close()\n",
    "\thandle = open(\"saved_W2.data\",\"rb\")\n",
    "\tW2 = np.load(handle)\n",
    "\thandle.close()\n",
    "\treturn [W1,W2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "Negative likelihood:  0\n",
      "Negative likelihood:  2.096752100625725\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-97-2486b4beb38c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m[\u001b[0m\u001b[0mW1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-96-3cb6adf5d6a6>\u001b[0m in \u001b[0;36mtrainer\u001b[1;34m(curW1, curW2, hidden_size)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                     \u001b[0mnegative_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerateSamples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext_book\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#create negative samplings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m                     \u001b[0mnll\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mperformDescent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcenter_book\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext_book\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m                     \u001b[1;31m#create negative log likelihood\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                     \u001b[1;31m#nll_results.append(nll)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-75-92c9511cc858>\u001b[0m in \u001b[0;36mperformDescent\u001b[1;34m(num_samples, learning_rate, center_token, context_words, W1, W2, negative_indices)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnegative_indices\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mv_j\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mW2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0msig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv_j\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[1;31m#update nll\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mnsig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv_j\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#1k iteration usually takes more than 10 minutes\n",
    "[W1,W2] = trainer()\n",
    "save_model(word_embeddings, proj_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
