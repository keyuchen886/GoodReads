{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict #defaultdict provides value of nonexist key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "REVIEW_DIR = \".\\data\\goodreads_reviews_comics_graphic.json\"\n",
    "BOOK_DIR = \".\\data\\goodreads_books_comics_graphic.json\"\n",
    "INTER_DIR = \".\\data\\goodreads_interactions_comics_graphic.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_review( record , table ):\n",
    "    \n",
    "    if record['rating']==0:\n",
    "        #do not perform any operation\n",
    "        return table\n",
    "    \n",
    "    \n",
    "    user_id = record['user_id']\n",
    "    book_id = record['book_id']\n",
    "    \n",
    "    \n",
    "    \n",
    "    if not user_id in table['users']:\n",
    "        table['users'].add(user_id)\n",
    "        #let table['books'] be a set\n",
    "    if not book_id in table.keys():  #check if this user has registered in our dataset\n",
    "        table[book_id] = set()\n",
    "\n",
    "    table[book_id].add(user_id) #register this book\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_records = 529532  #there are 529532 record in total\n",
    "index = 0 \n",
    "\n",
    "data = {'users':set()}\n",
    "\n",
    "#----------------------------\n",
    "#     run main\n",
    "#----------------------------\n",
    "\n",
    "with open(REVIEW_DIR) as fie:\n",
    "    for review in fie:\n",
    "        \n",
    "        if index > num_records:\n",
    "            fie.close()\n",
    "            break\n",
    "            \n",
    "        \n",
    "            \n",
    "        record = json.loads(review)  #load json as a dictionary\n",
    "        data = process_review(record, data)\n",
    "        \n",
    "        #print(i)\n",
    "        index+=1\n",
    "    \n",
    "    \n",
    "    \n",
    "    fie.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we then remove those books which are not popular\n",
    "data = [v for k,v in data.items() if len(v) > 1 ]\n",
    "data.pop(0) #also remove the first element, which is a set of all books\n",
    "# then compute how many books are in the dataset\n",
    "users = set()\n",
    "for v in data:\n",
    "    users = users.union(v)\n",
    "users = list(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_up = {users[i]:i for i in range(len(users))  }  \n",
    "#this is a mapping dictionary that map book id to a unique id\n",
    "user_code = {i:users[i] for i in range(len(users))}\n",
    "#this is a book code dictionary that map id to book id back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [ list(i) for i in data ] #change set to list\n",
    "data = [  [look_up[j] for j in i]  for i in data] #change all raw bookid to the id in look up table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51184"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = []\n",
    "train_y = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    for j in range(len(data[i])):\n",
    "        train_x.append( data[i][j] )\n",
    "        temp = data[i][:j]+data[i][j+1:]\n",
    "        train_y.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51184"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we prepare a keras data generator\n",
    "\n",
    "This is a key part because if we save all training x and y fully, we will have 2*num_samples*num_classes values to save however, we can use a generator to output each of them safely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, num_samples, n_classes, train_x, train_y, batch_size=32, dim=1, shuffle=True):\n",
    "        'Initialization'\n",
    "      \n",
    "        self.dim = dim #specify the input dimension\n",
    "        self.batch_size = batch_size  #the batch size\n",
    "        self.num_samples = num_samples  #how many ids are there in total\n",
    "        \n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end() #then call this method to kinda initialize it\n",
    "        \n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange( self.num_samples )\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor( self.num_samples / self.batch_size))\n",
    "    #---------------------------------------\n",
    "    #    now generate data samples\n",
    "    #---------------------------------------\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        #list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation( indexes )\n",
    "\n",
    "        return X, y\n",
    "\n",
    "\n",
    "    def __data_generation(self, list_IDs):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.array( [self.train_x[i] for i in list_IDs] )\n",
    "        y = np.zeros( (self.batch_size, self.n_classes), dtype=int ) \n",
    "\n",
    "        # Generate data\n",
    "        for i in range(len(list_IDs)):\n",
    "            # Store class\n",
    "            \n",
    "            for idx in self.train_y[ list_IDs[i]  ]:\n",
    "                y[i,idx] = 1\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "7421/7421 [==============================] - 875s 118ms/step - loss: 0.0855\n",
      "Epoch 2/3\n",
      "7421/7421 [==============================] - 873s 118ms/step - loss: 0.0188\n",
      "Epoch 3/3\n",
      "7421/7421 [==============================] - 873s 118ms/step - loss: 0.0180\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x172bf9fcd48>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "# Generators\n",
    "num_samples = len(train_x)\n",
    "num_classes = len(users)\n",
    "num_dimension = 100\n",
    "\n",
    "\n",
    "training_generator = DataGenerator(num_samples, num_classes,train_x,train_y, batch_size=64)\n",
    "\n",
    "# Design model\n",
    "model = keras.models.Sequential()\n",
    "embedding = keras.layers.Embedding( num_classes, num_dimension, input_length=1  ) \n",
    "#the input is one book of a user's like\n",
    "model.add(embedding)\n",
    "model.add(keras.layers.Flatten())  \n",
    "#since the input has only input_lengh being one, flatten it does not change everythingg and it's necessary\n",
    "model.add(keras.layers.Dense(num_classes,activation='sigmoid')) \n",
    "#our train_y is the other books this user likes. Hence its a vector of zeros and ones\n",
    "#one specify the books he like.\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "# Train model on dataset\n",
    "model.fit_generator(generator=training_generator, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51184, 100)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weights[0].numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = embedding.weights[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = { k:list( weights[v] ) for k,v in look_up.items()   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"user_embedding.firefire\",'wb') as file:\n",
    "    pickle.dump(test,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('user_embedding.firefire','rb') as f:\n",
    "    x = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
