{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict #defaultdict provides value of nonexist key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "REVIEW_DIR = \".\\data\\goodreads_reviews_comics_graphic.json\"\n",
    "BOOK_DIR = \".\\data\\goodreads_books_comics_graphic.json\"\n",
    "INTER_DIR = \".\\data\\goodreads_interactions_comics_graphic.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We need a user table which contains all the books he rated that has >=3\n",
    "# We need a list of all books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_review( record , table ):\n",
    "    \n",
    "    if record['rating']==0:\n",
    "        #do not perform any operation\n",
    "        return table\n",
    "    \n",
    "    \n",
    "    user_id = record['user_id']\n",
    "    book_id = record['book_id']\n",
    "    \n",
    "    \n",
    "    \n",
    "    if not book_id in table['books']:\n",
    "        table['books'].add(book_id)\n",
    "        #let table['books'] be a set\n",
    "    if not user_id in table.keys():  #check if this user has registered in our dataset\n",
    "        table[user_id] = set()\n",
    "\n",
    "    table[user_id].add(book_id) #register this book\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_records = 529532  #there are 529532 record in total\n",
    "index = 0 \n",
    "\n",
    "data = {'books':set()}\n",
    "\n",
    "#----------------------------\n",
    "#     run main\n",
    "#----------------------------\n",
    "\n",
    "with open(REVIEW_DIR) as fie:\n",
    "    for review in fie:\n",
    "        \n",
    "        if index > num_records:\n",
    "            fie.close()\n",
    "            break\n",
    "            \n",
    "        \n",
    "            \n",
    "        record = json.loads(review)  #load json as a dictionary\n",
    "        data = process_review(record, data)\n",
    "        \n",
    "        #print(i)\n",
    "        index+=1\n",
    "    \n",
    "    \n",
    "    \n",
    "    fie.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# we then need to build a look up dictionary for our books\n",
    "# we would first want to remove users with only one comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we then remove those user who only rates one book.\n",
    "data = [v for k,v in data.items() if len(v) > 1 ]\n",
    "data.pop(0) #also remove the first element, which is a set of all books\n",
    "# then compute how many books are in the dataset\n",
    "books = set()\n",
    "for v in data:\n",
    "    books = books.union(v)\n",
    "books = list(books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_up = {books[i]:i for i in range(len(books))  }  \n",
    "#this is a mapping dictionary that map book id to a unique id\n",
    "book_code = {i:books[i] for i in range(len(books))}\n",
    "#this is a book code dictionary that map id to book id back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we made up our training samples\n",
    "\n",
    "    Note here because the data is fairly large, we need a datagenerator to feed neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [list(i) for i in data] #change set to list\n",
    "data = [  [look_up[j] for j in i]  for i in data] #change all raw bookid to the id in look up table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [item for sublist in data for item in sublist] \n",
    "#flatten the data to a corpus, and we will use this corpus to do a counter job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1088, 69578, 30750, 63458, 20579]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookcounts = Counter(corpus)\n",
    "uniquebooks = np.unique(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negativeSampleTable(uniqueWords, wordcounts, exp_power=0.75): #exp_power is the default value\n",
    "    #global wordcounts\n",
    "    #... stores the normalizing denominator (count of all tokens, each count raised to exp_power)\n",
    "    max_exp_count = 0  #this is the sum of total weights\n",
    "    \n",
    "    print (\"Generating exponentiated count vectors\")\n",
    "    #... (TASK) for each uniqueWord, compute the frequency of that word to the power of exp_power\n",
    "    #... store results in exp_count_array.\n",
    "    exp_count_array = [wordcounts[i]**exp_power for i in wordcounts]\n",
    "    max_exp_count = sum(exp_count_array)\n",
    "\n",
    "    print (\"Generating distribution\")\n",
    "\n",
    "    #... (TASK) compute the normalized probabilities of each term.\n",
    "    #... using exp_count_array, normalize each value by the total value max_exp_count so that\n",
    "    #... they all add up to 1. Store this corresponding array in prob_dist\n",
    "    prob_dist = [np.float(i/max_exp_count) for i in exp_count_array]\n",
    "    #print(sum(prob_dist))\n",
    "\n",
    "\n",
    "    print (\"Filling up sampling table\")\n",
    "    #... (TASK) create a dict of size table_size where each key is a sequential number and its value is a one-hot index\n",
    "    #... the number of sequential keys containing the same one-hot index should be proportional to its prob_dist value\n",
    "    #... multiplied by table_size. This table should be stored in cumulative_dict.\n",
    "    #... we do this for much faster lookup later on when sampling from this table.\n",
    "\n",
    "    table_size = 1e7\n",
    "    counter=0 #this is to specify the index of array\n",
    "    #note if we do a for loop to print keys in a dict, it will preserve the same order for the same dict\n",
    "    #hence, we the order of above array prob_dist is the same as we print the wordcount dict\n",
    "    table_place = 0  #this is the sequential number we are current in among the dictionary cumulative_dict\n",
    "    cumulative_dict=dict()\n",
    "    \n",
    "    for key in wordcounts:\n",
    "        prob = prob_dist[counter]  #this will be the probability that it get sampled\n",
    "        \n",
    "        sub_table = round(prob*table_size) #round it to an int, which is how many keys in the table will have this index\n",
    "        \n",
    "        onehot_index = key\n",
    "        \n",
    "        for i in range(table_place,table_place+sub_table):  #we will make sub_table number of keys, they all have the same index.\n",
    "            cumulative_dict[i] = onehot_index\n",
    "        table_place = table_place+sub_table\n",
    "        counter+=1\n",
    "    \n",
    "\n",
    "    return cumulative_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating exponentiated count vectors\n",
      "Generating distribution\n",
      "Filling up sampling table\n"
     ]
    }
   ],
   "source": [
    "samplingTable = negativeSampleTable(uniquebooks, bookcounts)\n",
    "#samplingtable is a table that given a number between 0,10000 to will output the corresponding word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSamples(context_idx, num_samples):\n",
    "    #context_id is a list of ids which should not be negative sampled,\n",
    "    #num_samples is the total negative sample we want to do\n",
    "    global samplingTable\n",
    "    results = []\n",
    "\n",
    "    n=len(samplingTable)-1  #hence, we choose an int from 0 to n, which is the key of the sampling table\n",
    "    #... (TASK) randomly sample num_samples token indices from samplingTable.\n",
    "    #... don't allow the chosen token to be context_idx.\n",
    "    #... append the chosen indices to results\n",
    "    for i in range(num_samples):\n",
    "        index = context_idx #first make an index that will go the while loop       \n",
    "        while index in [context_idx]:  #this while loop will stop untile a sampled index is not in the context_idx\n",
    "            quantile = random.randint(0,n)\n",
    "            index = samplingTable[quantile]\n",
    "        #to get multiple negative samples, we will then add the selected samples into the context_idx\n",
    "        #this is like a sampling without replacement\n",
    "        results.append(index)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performDescent(num_samples, learning_rate, center_token, context_words,W1,W2,negative_indices):\n",
    "    # sequence chars was generated from the mapped sequence in the core code\n",
    "    nll_new = 0\n",
    "    #... (TASK) implement gradient descent. Find the current context token from context_words\n",
    "    #... and the associated negative samples from negative_indices. Run gradient descent on both\n",
    "    #... weight matrices W1 and W2.\n",
    "    #... compute the total negative log-likelihood and store this in nll_new.\n",
    "    #... You don't have to use all the input list above, feel free to change them\n",
    "    \n",
    "    voca = W1.shape[0]\n",
    "    hidden_size = W1.shape[1]\n",
    "    \n",
    "    #let's first train context_words\n",
    "\n",
    "    h = W1[center_token]\n",
    "    new_h = np.copy(h)\n",
    "    \n",
    "    #first let's solve for context_word\n",
    "    v_j = np.copy(W2[context_words])\n",
    "    sig = sigmoid( np.dot(v_j,np.transpose(h)) )\n",
    "    #minuse 1 because we are daling with context_words\n",
    "    #update log likelihood\n",
    "    nll_new-=np.log(sig)\n",
    "    #then update W1 \n",
    "    new_h -= learning_rate*(sig-1)*v_j\n",
    "    #then update the projection matrix\n",
    "    v_j = v_j - learning_rate*(sig-1)*h  \n",
    "    \n",
    "    W2[context_words]=v_j\n",
    "    \n",
    "    \n",
    "    #then we slove for negative words\n",
    "    for j in negative_indices:\n",
    "        v_j = W2[j]\n",
    "        sig = sigmoid( np.dot(v_j,np.transpose(h)) )\n",
    "        #update nll        \n",
    "        nsig = sigmoid(-np.dot(v_j,np.transpose(h)))\n",
    "        nll_new -= np.log(nsig)\n",
    "        #update W1\n",
    "        new_h -= learning_rate*sig*v_j\n",
    "        v_j = v_j - learning_rate*sig*h\n",
    "        W2[j] = v_j\n",
    "    #finally update h\n",
    "    W1[center_token] = new_h\n",
    "    \n",
    "\n",
    "    return nll_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(curW1 = None, curW2 = None, hidden_size=100):\n",
    "    global data #data is a list of list, in which each element is the id of that book.\n",
    "    \n",
    "    #... set the training parameters\n",
    "    epochs = 3\n",
    "    num_samples = 2\n",
    "    learning_rate = 0.05\n",
    "    nll = 0\n",
    "    iternum = 0\n",
    "    \n",
    "    book_size = len(look_up)\n",
    "    nll_results = []\n",
    "    \n",
    "    if curW1==None:\n",
    "        #np_randcounter += 1\n",
    "        W1 = np.random.uniform(-.5, .5, size=(book_size, hidden_size))\n",
    "        #print(W1)\n",
    "        W2 = np.random.uniform(-.5, .5, size=(book_size, hidden_size))\n",
    "        #print(W2)\n",
    "    else:\n",
    "        #... initialized from pre-loaded file\n",
    "        W1 = curW1\n",
    "        W2 = curW2  \n",
    "    \n",
    "    #now we start training:\n",
    "    for epc in range(epochs):\n",
    "        #how many epochs we need to run\n",
    "        print(\"epoch {epc}\".format(epc = str(epc)))\n",
    "        iternum=0\n",
    "        for user in data:\n",
    "            \n",
    "            if iternum%1000==0:\n",
    "                print (\"Negative likelihood: \", nll)\n",
    "                nll_results.append(nll)\n",
    "                nll = 0\n",
    "            if iternum%10000==0:\n",
    "                save_model(W1,W2)\n",
    "            \n",
    "            iternum += 1\n",
    "            #user is a list of all books this user rated\n",
    "            temp_books=user\n",
    "            #then we need to run through every book in this books set\n",
    "            for book_index in range(len(user)):\n",
    "                \n",
    "                center_book = temp_books[book_index]\n",
    "                context_books = temp_books[:book_index]+temp_books[(book_index+1):] \n",
    "                \n",
    "                #this is all contexts books, then we need to perform descent for each of this context books\n",
    "                for context_book in context_books:\n",
    "                    \n",
    "                    negative_indices = generateSamples(context_book, num_samples) #create negative samplings\n",
    "                    nll=performDescent(num_samples, learning_rate, center_book, context_book, W1, W2, negative_indices)\n",
    "                    #create negative log likelihood\n",
    "                    #nll_results.append(nll)\n",
    "                    \n",
    "                \n",
    "                    \n",
    "    return [W1,W2]\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(W1,W2):\n",
    "\thandle = open(\"saved_W1.data\",\"wb+\")\n",
    "\tnp.save(handle, W1, allow_pickle=False)\n",
    "\thandle.close()\n",
    "\n",
    "\thandle = open(\"saved_W2.data\",\"wb+\")\n",
    "\tnp.save(handle, W2, allow_pickle=False)\n",
    "\thandle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "\thandle = open(\"saved_W1.data\",\"rb\")\n",
    "\tW1 = np.load(handle)\n",
    "\thandle.close()\n",
    "\thandle = open(\"saved_W2.data\",\"rb\")\n",
    "\tW2 = np.load(handle)\n",
    "\thandle.close()\n",
    "\treturn [W1,W2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "Negative likelihood:  0\n",
      "Negative likelihood:  8.943419470696215\n",
      "Negative likelihood:  2.8318675601876633\n",
      "Negative likelihood:  2.324756304433464\n",
      "Negative likelihood:  0.7061995172715936\n",
      "Negative likelihood:  2.7092771246062393\n",
      "Negative likelihood:  1.4374306415239202\n",
      "Negative likelihood:  7.427740239383592\n",
      "Negative likelihood:  0.8563482901912824\n",
      "Negative likelihood:  2.772605326858002\n",
      "Negative likelihood:  2.478340140222707\n",
      "Negative likelihood:  2.616199370533028\n",
      "Negative likelihood:  1.2374100276670355\n",
      "Negative likelihood:  0.9475555930195149\n",
      "Negative likelihood:  3.219496509711116\n",
      "Negative likelihood:  5.8559455527996365\n",
      "Negative likelihood:  1.1658120608369935\n",
      "Negative likelihood:  1.7359210849166278\n",
      "Negative likelihood:  0.3904922835614683\n",
      "Negative likelihood:  0.4651917465619934\n",
      "Negative likelihood:  1.469536916919807\n",
      "Negative likelihood:  4.046177808516103\n",
      "Negative likelihood:  2.552448411882127\n",
      "Negative likelihood:  8.043885002928004\n",
      "Negative likelihood:  2.1408455397211674\n",
      "Negative likelihood:  1.034063296653136\n",
      "Negative likelihood:  0.021473453263979102\n",
      "Negative likelihood:  0.020163654417282274\n",
      "Negative likelihood:  2.3213514726212603\n",
      "Negative likelihood:  5.317540896970818\n",
      "Negative likelihood:  2.3084090108948865\n",
      "epoch 1\n",
      "Negative likelihood:  0.5525222436577348\n",
      "Negative likelihood:  10.777683473345013\n",
      "Negative likelihood:  1.7146155885402043\n",
      "Negative likelihood:  0.3501223209752131\n",
      "Negative likelihood:  1.5198573083081852\n",
      "Negative likelihood:  1.2678973300615228\n",
      "Negative likelihood:  1.8717866843070443\n",
      "Negative likelihood:  3.98566370996642\n",
      "Negative likelihood:  1.0358070415216465\n",
      "Negative likelihood:  1.7225026163556687\n",
      "Negative likelihood:  2.2463955589547338\n",
      "Negative likelihood:  2.4766815765568837\n",
      "Negative likelihood:  2.4723518793969075\n",
      "Negative likelihood:  1.590353141866231\n",
      "Negative likelihood:  2.137248461637868\n",
      "Negative likelihood:  11.403969322051339\n",
      "Negative likelihood:  0.5381826762596265\n",
      "Negative likelihood:  2.1663757332244207\n",
      "Negative likelihood:  1.537546388720824\n",
      "Negative likelihood:  0.19575118476253478\n",
      "Negative likelihood:  1.5243827807626689\n",
      "Negative likelihood:  2.3541735514154407\n",
      "Negative likelihood:  0.9636684168500323\n",
      "Negative likelihood:  7.022298753187652\n",
      "Negative likelihood:  3.7819745847222173\n",
      "Negative likelihood:  2.684233662087855\n",
      "Negative likelihood:  0.5349137536873898\n",
      "Negative likelihood:  0.02850037174506398\n",
      "Negative likelihood:  4.336203341989405\n",
      "Negative likelihood:  3.9434429196920933\n",
      "Negative likelihood:  1.7474262655717647\n",
      "epoch 2\n",
      "Negative likelihood:  0.0631680304987352\n",
      "Negative likelihood:  9.322619751944005\n",
      "Negative likelihood:  2.744746621289399\n",
      "Negative likelihood:  1.3968262910707074\n",
      "Negative likelihood:  2.9907028238996713\n",
      "Negative likelihood:  1.7167814802562364\n",
      "Negative likelihood:  0.9903751212365335\n",
      "Negative likelihood:  4.828470014221213\n",
      "Negative likelihood:  0.5356424595338712\n",
      "Negative likelihood:  2.454388775811532\n",
      "Negative likelihood:  2.8276287297350677\n",
      "Negative likelihood:  1.736513849536375\n",
      "Negative likelihood:  2.9764409027689\n",
      "Negative likelihood:  0.7636746094399595\n",
      "Negative likelihood:  2.1432447917752717\n",
      "Negative likelihood:  6.949869841283818\n",
      "Negative likelihood:  0.7712737562701772\n",
      "Negative likelihood:  2.2673995991954317\n"
     ]
    }
   ],
   "source": [
    "#1k iteration usually takes more than 10 minutes\n",
    "[W1,W2] = trainer()\n",
    "save_model(W1,W2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
