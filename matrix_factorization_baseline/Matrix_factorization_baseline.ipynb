{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This method will function as a baseline model to compare with our current embedding method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/user_embedding.firefire', \"rb\") as file:\n",
    "    user_embedding = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51184"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( user_embedding.keys() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/book_embedding.firefire', \"rb\") as file:\n",
    "    book_embedding = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86276"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( book_embedding.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's generate training and testing samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = list(user_embedding.keys())\n",
    "user_lookup = {users[i]:i for i in range(len(users))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = list(book_embedding.keys())\n",
    "book_lookup = {books[i]:i for i in range(len(books))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del users,books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_book = []\n",
    "test_x_user = []\n",
    "test_y = []\n",
    "train_x_book = []\n",
    "train_x_user = []\n",
    "train_y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(9001)\n",
    "rand_nums = [np.random.randint(5) for _ in range(515595)]\n",
    "with open('./data/goodreads_reviews_comics_graphic.json','r') as file:\n",
    "    index=0\n",
    "    for review in file:\n",
    "        record = json.loads(review)\n",
    "        if record['rating']!=0:\n",
    "            flag = rand_nums[index]\n",
    "            if flag==4:\n",
    "                #treat it as a test data\n",
    "                try:\n",
    "                    book_id = book_lookup[record['book_id']]\n",
    "                    user_id = user_lookup[record['user_id']]\n",
    "                    test_x_book.append(book_id )\n",
    "                    test_x_user.append(user_id)\n",
    "                    test_y.append( record['rating'])\n",
    "                    index+=1\n",
    "                except:\n",
    "                    continue\n",
    "            else: #we save this pair\n",
    "                try:\n",
    "                    book_id = book_lookup[record['book_id']]\n",
    "                    user_id = user_lookup[record['user_id']]\n",
    "                    train_x_book.append(book_id )\n",
    "                    train_x_user.append(user_id)\n",
    "                    train_y.append( record['rating'])\n",
    "                    index+=1\n",
    "                except:\n",
    "                    continue\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with bias and regularization\n",
    "\n",
    "Note keras automatically has the bias but we also want a regularization on the bias so we add it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_books=len(book_lookup)\n",
    "num_users = len(user_lookup)\n",
    "num_dimension=100\n",
    "alpha=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = keras.Input(shape=(None,), name=\"book_input\")\n",
    "input_embe1 = keras.layers.Embedding( num_books, num_dimension, activity_regularizer=keras.regularizers.l2(alpha))(input1)\n",
    "flat_embe1 = keras.layers.Flatten()(input_embe1)\n",
    "\n",
    "input2 = keras.Input(shape=(None,),name=\"user_input\")\n",
    "input_embe2 = keras.layers.Embedding( num_users, num_dimension, activity_regularizer=keras.regularizers.l2(alpha))(input2)\n",
    "flat_embe2 = keras.layers.Flatten()(input_embe2)\n",
    "dot = keras.layers.Dot(axes=1)([flat_embe1,flat_embe2])\n",
    "\n",
    "#adding bias term\n",
    "bias1 = keras.layers.Embedding( num_books, 1, activity_regularizer=keras.regularizers.l2(alpha))(input1)\n",
    "flat_bias1 = keras.layers.Flatten()(bias1)\n",
    "bias2 = keras.layers.Embedding( num_books, 1, activity_regularizer=keras.regularizers.l2(alpha))(input2)\n",
    "flat_bias2 = keras.layers.Flatten()(bias2)\n",
    "\n",
    "\n",
    "#the final prediction is the addition of dot product, and two bias terms\n",
    "output = keras.layers.Add()([dot,flat_bias1,flat_bias2])\n",
    "\n",
    "model= keras.models.Model(inputs=[input1,input2],outputs=[output])\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"keras_factor_model.h5\",\n",
    "save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=5,\n",
    "restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 412369 samples, validate on 103223 samples\n",
      "Epoch 1/20\n",
      "412369/412369 [==============================] - 801s 2ms/sample - loss: 10.4654 - val_loss: 5.6591\n",
      "Epoch 2/20\n",
      "412369/412369 [==============================] - 823s 2ms/sample - loss: 3.3989 - val_loss: 3.3443\n",
      "Epoch 3/20\n",
      "412369/412369 [==============================] - 797s 2ms/sample - loss: 1.5786 - val_loss: 2.8088\n",
      "Epoch 4/20\n",
      "412369/412369 [==============================] - 822s 2ms/sample - loss: 0.8328 - val_loss: 2.6529\n",
      "Epoch 5/20\n",
      "412369/412369 [==============================] - 824s 2ms/sample - loss: 0.4863 - val_loss: 2.5946\n",
      "Epoch 6/20\n",
      "412369/412369 [==============================] - 830s 2ms/sample - loss: 0.3347 - val_loss: 2.5539\n",
      "Epoch 7/20\n",
      "412369/412369 [==============================] - 811s 2ms/sample - loss: 0.2760 - val_loss: 2.5244\n",
      "Epoch 8/20\n",
      "412369/412369 [==============================] - 814s 2ms/sample - loss: 0.2575 - val_loss: 2.5042\n",
      "Epoch 9/20\n",
      "412369/412369 [==============================] - 845s 2ms/sample - loss: 0.2496 - val_loss: 2.4813\n",
      "Epoch 10/20\n",
      "412369/412369 [==============================] - 844s 2ms/sample - loss: 0.2433 - val_loss: 2.4591\n",
      "Epoch 11/20\n",
      "412369/412369 [==============================] - 817s 2ms/sample - loss: 0.2385 - val_loss: 2.4458\n",
      "Epoch 12/20\n",
      "412369/412369 [==============================] - 816s 2ms/sample - loss: 0.2345 - val_loss: 2.4259\n",
      "Epoch 13/20\n",
      "412369/412369 [==============================] - 821s 2ms/sample - loss: 0.2314 - val_loss: 2.4119\n",
      "Epoch 14/20\n",
      "412369/412369 [==============================] - 815s 2ms/sample - loss: 0.2285 - val_loss: 2.3978\n",
      "Epoch 15/20\n",
      "412369/412369 [==============================] - 801s 2ms/sample - loss: 0.2259 - val_loss: 2.3864\n",
      "Epoch 16/20\n",
      "412369/412369 [==============================] - 829s 2ms/sample - loss: 0.2234 - val_loss: 2.3757\n",
      "Epoch 17/20\n",
      "412369/412369 [==============================] - 829s 2ms/sample - loss: 0.2208 - val_loss: 2.3648\n",
      "Epoch 18/20\n",
      "412369/412369 [==============================] - 831s 2ms/sample - loss: 0.2196 - val_loss: 2.3588\n",
      "Epoch 19/20\n",
      "412369/412369 [==============================] - 872s 2ms/sample - loss: 0.2174 - val_loss: 2.3479\n",
      "Epoch 20/20\n",
      "412369/412369 [==============================] - 842s 2ms/sample - loss: 0.2158 - val_loss: 2.3385\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x170f3668c08>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit( ( np.array(train_x_book).reshape( (-1,1)), np.array(train_x_user).reshape( (-1,1)) ), np.array(train_y).reshape( (-1,1)), \n",
    "          callbacks=[checkpoint_cb, early_stopping_cb], \n",
    "          validation_data=(( np.array(test_x_book).reshape( (-1,1)), np.array(test_x_user).reshape( (-1,1)) ), np.array(test_y).reshape( (-1,1))),\n",
    "          epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 412369 samples, validate on 103223 samples\n",
      "Epoch 1/20\n",
      "  8896/412369 [..............................] - ETA: 12:19 - loss: 0.2122"
     ]
    }
   ],
   "source": [
    "model.fit( ( np.array(train_x_book).reshape( (-1,1)), np.array(train_x_user).reshape( (-1,1)) ), np.array(train_y).reshape( (-1,1)), \n",
    "          callbacks=[checkpoint_cb, early_stopping_cb], \n",
    "          validation_data=(( np.array(test_x_book).reshape( (-1,1)), np.array(test_x_user).reshape( (-1,1)) ), np.array(test_y).reshape( (-1,1))),\n",
    "          epochs=20, batch_size=64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
